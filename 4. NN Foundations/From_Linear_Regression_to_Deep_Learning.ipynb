{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq_3h0Be5nDk"
      },
      "source": [
        "# **Regression with Neural Networks ‚Äì From Linear Regression to Deep Learning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zC02YQE5sNM"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# üì¶ Installation (Only for Google Colab)\n",
        "# ------------------------------------------\n",
        "!pip install -q pandas matplotlib seaborn scikit-learn tensorflow pydot\n",
        "\n",
        "# Required for model visualization\n",
        "!apt-get -qq install graphviz\n",
        "\n",
        "# ------------------------------------------\n",
        "# üîß Import Libraries\n",
        "# ------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn modules for regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# TensorFlow for ANN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Plot settings\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6dz5n2Z6ubG"
      },
      "source": [
        "# üß† Regression for Business: From Linear Regression to Deep Neural Networks\n",
        "\n",
        "## üìä Business Case: California Housing Price Prediction\n",
        "The goal is to predict **house prices** in California based on features like location, median income, number of rooms, and more. Using historical housing data, our goal is to build a **predictive model** to estimate the **price of a house** for each district.\n",
        "\n",
        "---\n",
        "## üßÆ Problem Type: Regression\n",
        "We are solving a **supervised learning** problem with continuous output:\n",
        "$$\n",
        "y_i \\in \\mathbb{R}, \\quad \\hat{y}_i \\in \\mathbb{R}\n",
        "$$\n",
        "Where:\n",
        "- $y_i$ represents the **true price** of the house\n",
        "- $\\hat{y}_i$ represents the **predicted price** of the house\n",
        "\n",
        "---\n",
        "## üß± Modeling Strategy\n",
        "We will build and compare the following models:\n",
        "\n",
        "| Step | Model | Description |\n",
        "\n",
        "|------|--------------------------|-------------|\n",
        "\n",
        "| 1Ô∏è‚É£ | Linear Regression | Simple linear model for price prediction based on features |\n",
        "\n",
        "| 2Ô∏è‚É£ | Basic Neural Network | One hidden layer to capture non-linear relationships |\n",
        "\n",
        "| 3Ô∏è‚É£ | Deep Neural Network | Multiple layers for deeper representations and more complex patterns |\n",
        "\n",
        "---\n",
        "## üéØ Learning Objectives\n",
        "By completing this notebook, you will:\n",
        "‚úÖ Understand how to formulate a regression task for predicting house prices\n",
        "‚úÖ Learn how linear regression compares to neural networks for price prediction\n",
        "‚úÖ Implement and train models in `scikit-learn` and `TensorFlow/Keras`\n",
        "‚úÖ Evaluate models using Mean Squared Error (MSE) and R¬≤ score\n",
        "‚úÖ Analyze model performance in a real-world business context\n",
        "\n",
        "---\n",
        "## üì¶ Dataset\n",
        "- **Source**: California Housing Prices dataset (from the UCI Machine Learning Repository)\n",
        "- **Target**: `Median House Value` (continuous variable)\n",
        "- **Features**: Location (longitude, latitude), housing median age, total rooms, total bedrooms, population, households, median income, and ocean proximity\n",
        "\n",
        "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
        "\n",
        "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
        "\n",
        "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
        "\n",
        "4. totalRooms: Total number of rooms within a block\n",
        "\n",
        "5. totalBedrooms: Total number of bedrooms within a block\n",
        "\n",
        "6. population: Total number of people residing within a block\n",
        "\n",
        "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
        "\n",
        "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "\n",
        "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
        "\n",
        "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
        "\n",
        "- **Challenge**: Data preprocessing and feature scaling required, plus comparison between linear and neural network models\n",
        "\n",
        "Let's begin by loading and exploring the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f6K5H7v6vfE"
      },
      "outputs": [],
      "source": [
        "# üì• Download the California Housing Prices dataset\n",
        "!wget https://raw.githubusercontent.com/sonarsushant/California-House-Price-Prediction/master/housing.csv -q\n",
        "\n",
        "# üì• Load the California Housing dataset\n",
        "df = pd.read_csv('housing.csv')\n",
        "\n",
        "# üßæ Inspect the first few rows\n",
        "print(\"üìå First five rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# üìè Dataset shape\n",
        "print(f\"\\n‚úÖ Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "# üìä Data types\n",
        "print(\"\\nüîç Data types of each column:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# üîé Missing values\n",
        "print(\"\\n‚ùó Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# üìä Class distribution (for regression tasks, you can look at the price distribution)\n",
        "print(\"\\nüìä Price distribution:\")\n",
        "print(df['median_house_value'].describe())\n",
        "\n",
        "# üß† Interpretation\n",
        "print(\n",
        "\"\"\"\n",
        "üß† Notes:\n",
        "- The dataset contains housing data from California districts.\n",
        "- The target variable is 'median_house_value', which represents the median price of houses in a district.\n",
        "- Features include location, number of rooms, median income, and more.\n",
        "- The dataset is relatively clean with no missing values detected, making it ready for modeling.\n",
        "- As the target is continuous (house prices), we will treat this as a regression task.\n",
        "\"\"\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FikGDVt8Ewd"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìä PRICE DISTRIBUTION\n",
        "# ------------------------------------------\n",
        "# Inspect the distribution of house prices\n",
        "price_distribution = df['median_house_value'].describe()\n",
        "print(\"üßÆ House Price Distribution:\")\n",
        "display(price_distribution)\n",
        "\n",
        "# Plot house price distribution (log scale to handle skewness)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.histplot(df['median_house_value'], kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of House Prices\")\n",
        "plt.xlabel(\"House Price\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.yscale('log')  # Log scale for better visualization of skewed data\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìà NUMERICAL FEATURES ANALYSIS\n",
        "# ------------------------------------------\n",
        "# Plot distribution of some numerical features (e.g., 'Median Income' and 'Household Size')\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot distribution of median income\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(data=df, x='median_income', kde=True, color='green', bins=50)\n",
        "plt.title(\"Distribution of Median Income\")\n",
        "plt.xlabel(\"Median Income\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot distribution of house age\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(data=df, x='housing_median_age', kde=True, color='red', bins=50)\n",
        "plt.title(\"Distribution of Housing Median Age\")\n",
        "plt.xlabel(\"Housing Median Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìä FEATURE DISTRIBUTIONS FOR REGRESSION\n",
        "# ------------------------------------------\n",
        "# Select a subset of features for boxplot analysis\n",
        "sample_features = ['longitude', 'latitude', 'median_income', 'housing_median_age']\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "for i, feature in enumerate(sample_features):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    sns.boxplot(x='ocean_proximity', y=feature, data=df)  # 'ocean_proximity' is categorical\n",
        "    plt.title(f\"Distribution of {feature} by Ocean Proximity\")\n",
        "    plt.xlabel(\"Ocean Proximity\")\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# üîó CORRELATION MATRIX FOR TOP FEATURES\n",
        "# ------------------------------------------\n",
        "# Select only numerical columns for correlation calculation\n",
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Plot correlation matrix for numerical features\n",
        "plt.figure(figsize=(12, 10))\n",
        "# Calculate correlation only for numerical features\n",
        "corr_matrix = df[numerical_columns].corr()\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "            vmin=-1, vmax=1, center=0, fmt='.2f')\n",
        "plt.title(\"Correlation Matrix of Select Features\")\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìà FEATURES MOST CORRELATED WITH HOUSE PRICE\n",
        "# ------------------------------------------\n",
        "plt.figure(figsize=(10, 8))\n",
        "# Calculate correlation with 'median_house_value' for numerical features\n",
        "corr_with_price = df[numerical_columns].corr()['median_house_value'].sort_values(ascending=False)\n",
        "corr_with_price = corr_with_price[corr_with_price.index != 'median_house_value']  # Remove self-correlation\n",
        "top_corr = corr_with_price.head(10)\n",
        "bottom_corr = corr_with_price.tail(10)\n",
        "\n",
        "# Combine top positive and negative correlations\n",
        "important_corrs = pd.concat([top_corr, bottom_corr])\n",
        "colors = ['red' if x > 0 else 'blue' for x in important_corrs.values]\n",
        "\n",
        "sns.barplot(x=important_corrs.values, y=important_corrs.index, palette=colors)\n",
        "plt.title(\"Features Most Correlated with House Price\")\n",
        "plt.xlabel(\"Correlation Coefficient\")\n",
        "plt.grid(True)\n",
        "plt.axvline(x=0, color='black', linestyle='--')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahAWtNgU-pMT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------\n",
        "# üßπ Data Preprocessing\n",
        "# ------------------------------------------\n",
        "# Scale the 'median_house_value' and other numerical features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a copy of dataframe for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Extract and scale 'median_house_value' and other numerical features separately\n",
        "price_scaler = StandardScaler()\n",
        "df_processed['median_house_value_scaled'] = price_scaler.fit_transform(df_processed[['median_house_value']])\n",
        "\n",
        "# Features like 'median_income', 'housing_median_age' can also be standardized\n",
        "income_scaler = StandardScaler()\n",
        "df_processed['median_income_scaled'] = income_scaler.fit_transform(df_processed[['median_income']])\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìä Mean comparison: Ocean Proximity vs House Prices\n",
        "# ------------------------------------------\n",
        "print(\"üîç Average values for key features by Ocean Proximity:\\n\")\n",
        "numeric_summary = df.groupby('ocean_proximity')[['median_house_value', 'median_income', 'housing_median_age', 'total_rooms', 'total_bedrooms']].mean().round(4)\n",
        "display(numeric_summary)\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìä Statistical Summary of Ocean Proximity vs House Price\n",
        "# ------------------------------------------\n",
        "# Create a function to generate statistical summaries for house prices\n",
        "def get_stats_by_group(df, feature):\n",
        "    stats = df.groupby('ocean_proximity')[feature].agg(['mean', 'median', 'std', 'min', 'max']).round(4)\n",
        "    return stats\n",
        "\n",
        "# Generate statistics for 'median_house_value'\n",
        "house_price_stats = get_stats_by_group(df, 'median_house_value')\n",
        "print(\"\\nüìä House Price Statistics by Ocean Proximity:\\n\")\n",
        "display(house_price_stats)\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìà House Price Percentiles by Ocean Proximity\n",
        "# ------------------------------------------\n",
        "print(\"\\nüìä House Price Percentiles by Ocean Proximity:\\n\")\n",
        "percentiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
        "percentile_stats = df.groupby('ocean_proximity')['median_house_value'].quantile(percentiles).unstack().round(2)\n",
        "display(percentile_stats)\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìà Feature Importance Analysis\n",
        "# ------------------------------------------\n",
        "# Exclude non-numeric columns (e.g., ocean_proximity) for correlation calculation\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate correlation with house price\n",
        "correlations = numeric_df.corr()['median_house_value'].sort_values(key=lambda x: abs(x), ascending=False)\n",
        "top_features = correlations.head(11).index.tolist()[1:]  # Exclude 'median_house_value' itself\n",
        "\n",
        "print(\"\\nüìå Top features correlated with House Price (absolute value):\\n\")\n",
        "display(correlations.head(11))\n",
        "\n",
        "# ------------------------------------------\n",
        "# üìä Feature Distributions: Ocean Proximity vs House Prices\n",
        "# ------------------------------------------\n",
        "# Create a dataframe with statistical differences\n",
        "feature_stats = pd.DataFrame(index=df.columns[1:-1])  # Skip 'ocean_proximity' and 'median_house_value'\n",
        "feature_stats['Ocean_Mean'] = df[df['ocean_proximity'] == 'Near Ocean'].mean()\n",
        "feature_stats['Inland_Mean'] = df[df['ocean_proximity'] == 'Inland'].mean()\n",
        "feature_stats['Mean_Diff'] = abs(feature_stats['Ocean_Mean'] - feature_stats['Inland_Mean'])\n",
        "feature_stats['Mean_Diff_Pct'] = (feature_stats['Mean_Diff'] / feature_stats['Ocean_Mean'].abs()) * 100\n",
        "\n",
        "# Sort by absolute percentage difference\n",
        "feature_stats = feature_stats.sort_values('Mean_Diff_Pct', ascending=False)\n",
        "\n",
        "print(\"\\nüìä Features with Largest Percentage Difference between Ocean Proximity:\\n\")\n",
        "display(feature_stats.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FaL1PEl9SjS"
      },
      "source": [
        "# üßÆ Feature Standardization & Train-Test Split\n",
        "\n",
        "## üìè Standardization of Features\n",
        "\n",
        "### ‚ùì Why Standardize?\n",
        "\n",
        "Many machine learning models ‚Äî especially those based on **dot products** or **gradient descent** (like linear regression and neural networks) ‚Äî are sensitive to the **scale of input features**.\n",
        "\n",
        "Features with larger numerical ranges can:\n",
        "- Dominate the learning process\n",
        "- Slow down convergence\n",
        "- Cause unstable gradients during training\n",
        "\n",
        "To resolve this, we apply **z-score standardization**:\n",
        "\n",
        "$$\n",
        "z_j = \\frac{x_j - \\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $x_j$ is the original value of feature $j$\n",
        "- $\\mu_j$ is the **mean** of feature $j$ (from the training set only)\n",
        "- $\\sigma_j$ is the **standard deviation** of feature $j$\n",
        "\n",
        "This transforms the feature to have:\n",
        "- Mean ‚âà 0\n",
        "- Standard deviation = 1\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÇÔ∏è Train-Test Split\n",
        "\n",
        "To evaluate model performance fairly, we split our dataset into:\n",
        "\n",
        "- **Training set**: Used to train the model\n",
        "- **Test set**: Held-out data to assess generalization\n",
        "\n",
        "We typically use an 80/20 or 70/30 split.\n",
        "\n",
        "### Key Principle: No Information Leakage\n",
        "\n",
        "Standardization and preprocessing must use **only training data statistics**. Never use test data to compute scaling parameters. That's why we apply:\n",
        "\n",
        "1. `scaler.fit(X_train)` ‚Äî compute mean and std from the training data  \n",
        "2. `scaler.transform(X_train)` and `scaler.transform(X_test)` ‚Äî apply to both sets\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWHVX49-9mfd"
      },
      "outputs": [],
      "source": [
        "# üïµÔ∏è California Housing Price Prediction - Data Preprocessing\n",
        "\n",
        "# 1Ô∏è‚É£ Initial Data Inspection\n",
        "print(\"üîç Dataset Information:\")\n",
        "print(df.info())\n",
        "\n",
        "# 2Ô∏è‚É£ Check for Missing Values\n",
        "print(\"\\nüï≥Ô∏è Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 3Ô∏è‚É£ Basic Statistical Summary of Features\n",
        "print(\"\\nüìä Feature Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# 4Ô∏è‚É£ Preprocessing Steps\n",
        "# Separate features and target\n",
        "X = df.drop('median_house_value', axis=1)  # Target variable: 'median_house_value'\n",
        "y = df['median_house_value']\n",
        "\n",
        "# 5Ô∏è‚É£ Handle Potential Outliers and Scale Features\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select only the numerical columns for scaling (exclude categorical 'ocean_proximity')\n",
        "numerical_columns = X.select_dtypes(include=[np.number]).columns  # Select numerical columns only\n",
        "\n",
        "# Use RobustScaler to handle potential outliers (good for regression tasks with outliers)\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X[numerical_columns])  # Apply scaling only on numerical columns\n",
        "\n",
        "# Replace the original numerical columns with the scaled ones\n",
        "X[numerical_columns] = X_scaled\n",
        "\n",
        "# 6Ô∏è‚É£ Train-Test Split\n",
        "# We use a 80/20 split to ensure we have sufficient data for model evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True  # Important for randomization\n",
        ")\n",
        "\n",
        "# 7Ô∏è‚É£ Verify Split Characteristics\n",
        "print(\"\\nüî¢ Dataset Split:\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# üîü Additional Preprocessing Notes:\n",
        "# - Time column is not present in this dataset.\n",
        "# - No need for one-hot encoding since features are already numerical.\n",
        "# - RobustScaler used to minimize the impact of outliers in numerical features.\n",
        "# - Standard train-test split (80/20) applied, no need for stratified splitting as we are working with regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTZBE6ny_M8S"
      },
      "source": [
        "# üïµÔ∏è Linear Regression for House Price Prediction\n",
        "\n",
        "## üß† What is Linear Regression in House Price Prediction?\n",
        "Linear regression is a **linear model** used to predict the **price of a house** based on one or more features such as size, number of rooms, and location. It works by fitting a straight line (or hyperplane in multiple dimensions) to the data points to predict the target variable, which in this case is the house price.\n",
        "\n",
        "---\n",
        "## üî¢ Model Equation for House Price Prediction\n",
        "The model computes a **linear combination of house features**:\n",
        "$$\n",
        "y = \\mathbf{w}^\\top \\mathbf{x} + b\n",
        "$$\n",
        "Where:\n",
        "- $\\mathbf{x}$ represents the features of the house (e.g., size, number of rooms, etc.)\n",
        "- $\\mathbf{w}$ are the learned weights (coefficients for each feature)\n",
        "- $b$ is the bias term (intercept)\n",
        "\n",
        "The model predicts the house price ($y$) as a weighted sum of the features.\n",
        "\n",
        "---\n",
        "## üìâ Loss Function: Mean Squared Error (MSE)\n",
        "The **Mean Squared Error (MSE)** is commonly used as the loss function in linear regression:\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y}) = (y - \\hat{y})^2\n",
        "$$\n",
        "For a dataset of $N$ houses, we minimize the **average MSE**:\n",
        "$$\n",
        "J(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "$$\n",
        "Where:\n",
        "- $y^{(i)}$ is the true house price\n",
        "- $\\hat{y}^{(i)}$ is the predicted house price\n",
        "\n",
        "---\n",
        "## üéØ Optimization: Learning the Weights for House Price Prediction\n",
        "The goal is to find the **optimal weights** that minimize the loss function (MSE):\n",
        "$$\n",
        "J(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - (\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b))^2\n",
        "$$\n",
        "Where:\n",
        "- $\\hat{y}^{(i)} = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$ is the predicted house price for the $i$-th house\n",
        "\n",
        "---\n",
        "## üìâ Gradient Descent for House Price Prediction\n",
        "We use **gradient descent** to minimize the loss by iteratively updating the weights:\n",
        "### üí° Gradient Calculations\n",
        "**Gradient w.r.t. weights:**\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{w}} = -\\frac{2}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)}) \\cdot \\mathbf{x}^{(i)}\n",
        "$$\n",
        "**Gradient w.r.t. bias:**\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = -\\frac{2}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})\n",
        "$$\n",
        "\n",
        "---\n",
        "## üîÅ Parameter Update Rule\n",
        "At each iteration $t$, we update the weights and bias using the gradients calculated:\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)}) \\cdot \\mathbf{x}^{(i)}\n",
        "$$\n",
        "$$\n",
        "b^{(t+1)} = b^{(t)} - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})\n",
        "$$\n",
        "Where:\n",
        "- $\\eta$ is the learning rate (step size for updates)\n",
        "- $\\hat{y}^{(i)}$ is the predicted house price for the $i$-th house\n",
        "\n",
        "---\n",
        "## ‚öôÔ∏è Practical Considerations in House Price Prediction\n",
        "- **Feature Engineering**: Include relevant features like square footage, number of rooms, location, etc.\n",
        "- **Overfitting**: Be cautious of overfitting when using too many features (e.g., polynomial features).\n",
        "- **Model Evaluation**: Use metrics like **R-squared (R¬≤)**, **Mean Absolute Error (MAE)**, and **Mean Squared Error (MSE)** to assess model performance.\n",
        "\n",
        "---\n",
        "## üö® House Price Prediction Specifics\n",
        "‚úÖ Linear regression provides:\n",
        "- **Predicted House Prices** based on features\n",
        "- **Interpretability**: Coefficients represent the influence of each feature on the price\n",
        "- **Model Simplicity**: Works well with linear relationships\n",
        "\n",
        "---\n",
        "## üîÑ Connection to Neural Networks\n",
        "Linear regression can be viewed as a **single-layer neural network**:\n",
        "- Input layer consists of house features\n",
        "- Output layer predicts the house price using a linear combination of features\n",
        "- A foundational model for understanding more complex regression techniques, such as neural networks\n",
        "\n",
        "---\n",
        "‚úÖ In the next implementation, we'll apply **linear regression** to predict house prices, utilizing the features in the **California Housing dataset**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU7UzT_F_8-1"
      },
      "source": [
        "# üïµÔ∏è House Price Prediction: Parameter Interpretation & Evaluation Metrics\n",
        "\n",
        "## 1. üîç Coefficient Interpretation for House Price Prediction\n",
        "\n",
        "In **linear regression**, the model predicts the **price of a house** based on a **linear combination of features**:\n",
        "\n",
        "$$\n",
        "y = \\mathbf{w}^\\top \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{x}$ represents the features of the house (e.g., size, number of rooms, location, etc.)\n",
        "- $\\mathbf{w}$ are the learned weights (coefficients for each feature)\n",
        "- $b$ is the bias term (intercept)\n",
        "\n",
        "### Coefficient Interpretation\n",
        "\n",
        "- Each coefficient $w_j$ represents the **change in the predicted house price** for a one-unit increase in the feature $x_j$, **holding other features constant**.\n",
        "- For example, if $w_j = 200$, then for every additional unit (e.g., 1 square meter) of the feature $x_j$ (e.g., size of the house), the price increases by $200.\n",
        "  \n",
        "The bias term $b$ represents the predicted price when all features are zero (if it makes sense contextually).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. üßÆ Loss Function in House Price Prediction\n",
        "\n",
        "### Mean Squared Error (MSE)\n",
        "\n",
        "The loss function in linear regression is typically the **Mean Squared Error (MSE)**, which measures the average squared difference between the actual and predicted house prices:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y}) = (y - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "For a dataset of $N$ houses, the **MSE** is:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $y^{(i)}$ is the true house price\n",
        "- $\\hat{y}^{(i)}$ is the predicted house price\n",
        "\n",
        "---\n",
        "\n",
        "## 3. üìä R¬≤ and Adjusted R¬≤ in House Price Prediction\n",
        "\n",
        "### R¬≤ (Coefficient of Determination)\n",
        "\n",
        "R¬≤ measures the proportion of the variance in the target variable (house price) that is explained by the model:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^{N} (y^{(i)} - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $y^{(i)}$ is the actual house price\n",
        "- $\\hat{y}^{(i)}$ is the predicted house price\n",
        "- $\\bar{y}$ is the mean of the actual house prices\n",
        "\n",
        "**Interpretation**:\n",
        "- $R^2 = 1$ means perfect prediction\n",
        "- $R^2 = 0$ means the model does not explain any variance in the target\n",
        "- Negative $R^2$ means the model is worse than using the mean of the target variable\n",
        "\n",
        "### Adjusted R¬≤\n",
        "\n",
        "The **adjusted R¬≤** accounts for the number of features and is more suitable when comparing models with different numbers of features. It is calculated as:\n",
        "\n",
        "$$\n",
        "\\bar{R}^2 = 1 - \\left(1 - R^2\\right)\\frac{N - 1}{N - p - 1}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $N$ is the number of data points\n",
        "- $p$ is the number of features\n",
        "\n",
        "---\n",
        "\n",
        "## 4. ‚úÖ House Price Prediction Evaluation Metrics\n",
        "\n",
        "### 1. üìê **Mean Absolute Error (MAE)**\n",
        "\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y^{(i)} - \\hat{y}^{(i)}|\n",
        "$$\n",
        "\n",
        "**Interpretation**: MAE measures the **average absolute difference** between actual and predicted house prices. A lower MAE indicates better model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. üìâ **Mean Squared Error (MSE)**\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "$$\n",
        "\n",
        "**Interpretation**: MSE penalizes larger errors more heavily than MAE due to squaring the differences. A lower MSE indicates a model that is better at predicting house prices.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. üéØ **Root Mean Squared Error (RMSE)**\n",
        "\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2}\n",
        "$$\n",
        "\n",
        "**Interpretation**: RMSE is the square root of MSE and represents the typical size of prediction errors. A smaller RMSE indicates better model performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. ‚öñÔ∏è **Adjusted R¬≤**\n",
        "\n",
        "Adjusted R¬≤ adjusts R¬≤ for the number of predictors in the model, and is useful for model comparison:\n",
        "\n",
        "$$\n",
        "\\bar{R}^2 = 1 - \\left(1 - R^2\\right)\\frac{N - 1}{N - p - 1}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "## üí° House Price Prediction Specifics\n",
        "\n",
        "**Key Considerations**:\n",
        "- Data normalization and feature selection are crucial for high-dimensional datasets.\n",
        "- Outliers can significantly affect the model, and methods like **RobustScaler** should be used for scaling.\n",
        "- If the price prediction task is **regression**, metrics like **R¬≤**, **MAE**, and **RMSE** are appropriate.\n",
        "\n",
        "**Recommended Approach**:\n",
        "- Use multiple evaluation metrics to assess model performance.\n",
        "- Consider feature engineering (e.g., creating interaction terms, adding polynomial features).\n",
        "- Regularize the model (e.g., using Ridge or Lasso regression) to prevent overfitting when dealing with a large number of features.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ These metrics provide a **comprehensive evaluation** of house price prediction models, helping you understand their performance and make improvements where necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJXDCpf-BnEH"
      },
      "outputs": [],
      "source": [
        "# üïµÔ∏è House Price Prediction: Parameter Interpretation & Evaluation Metrics\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    explained_variance_score\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1Ô∏è‚É£ Load the Dataset\n",
        "df = pd.read_csv('housing.csv')\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2Ô∏è‚É£ Handle Missing Values\n",
        "# ------------------------------------------\n",
        "# Separate numerical and categorical columns\n",
        "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "categorical_columns = df.select_dtypes(include=[object]).columns\n",
        "\n",
        "# Impute missing values for numerical columns using mean\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Impute missing values for categorical columns using mode (most frequent)\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3Ô∏è‚É£ One-Hot Encode Categorical Columns\n",
        "# ------------------------------------------\n",
        "# One-hot encoding for categorical columns (e.g., 'ocean_proximity')\n",
        "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4Ô∏è‚É£ Separate Features and Target\n",
        "# ------------------------------------------\n",
        "X = df.drop('median_house_value', axis=1)  # Features\n",
        "y = df['median_house_value']  # Target variable\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5Ô∏è‚É£ Feature Scaling\n",
        "# ------------------------------------------\n",
        "# Use RobustScaler to handle potential outliers in numerical data\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # Apply RobustScaler to the numerical features\n",
        "\n",
        "# ------------------------------------------\n",
        "# 6Ô∏è‚É£ Train-Test Split (Train on train, Test on test)\n",
        "# ------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 7Ô∏è‚É£ Train the Linear Regression Model\n",
        "# ------------------------------------------\n",
        "# Initialize the linear regression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 8Ô∏è‚É£ Coefficients Interpretation\n",
        "# ------------------------------------------\n",
        "def analyze_coefficients(model, feature_names):\n",
        "    \"\"\"Analyze and visualize model coefficients\"\"\"\n",
        "    coefs = model.coef_\n",
        "\n",
        "    # Create DataFrame of coefficients\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': coefs\n",
        "    }).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "    print(\"üîç Top Features with Impact on House Price:\")\n",
        "    print(coef_df.head(10))  # Top 10 features\n",
        "\n",
        "    # Visualization of feature importances\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Coefficient', y='Feature', data=coef_df.head(10))\n",
        "    plt.title('Top 10 Features: Impact on House Price')\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return coef_df\n",
        "\n",
        "# Analyze coefficients\n",
        "coef_df = analyze_coefficients(lr, X.columns)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 9Ô∏è‚É£ Predictions and Evaluation Metrics\n",
        "# ------------------------------------------\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'Mean Absolute Error (MAE)': mean_absolute_error(y_test, y_pred),\n",
        "        'Mean Squared Error (MSE)': mean_squared_error(y_test, y_pred),\n",
        "        'Root Mean Squared Error (RMSE)': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R¬≤ (Coefficient of Determination)': r2_score(y_test, y_pred),\n",
        "        'Explained Variance': explained_variance_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"üö® House Price Prediction Performance:\")\n",
        "    for name, value in metrics.items():\n",
        "        print(f\"‚úÖ {name}: {value:.4f}\")\n",
        "\n",
        "    # Display metrics in a visualization-friendly format (Optional)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))\n",
        "    plt.title(\"Model Evaluation Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate the model\n",
        "model_metrics = evaluate_model(lr, X_test, y_test)\n",
        "\n",
        "# ------------------------------------------\n",
        "# üîü Residuals Plot\n",
        "\n",
        "def plot_residuals(y_test, y_pred):\n",
        "    \"\"\"Plot Residuals to assess the fit of the model\"\"\"\n",
        "    residuals = y_test - y_pred\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.scatterplot(x=y_pred, y=residuals)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.title(\"Residuals vs Predicted House Prices\")\n",
        "    plt.xlabel(\"Predicted House Price\")\n",
        "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot residuals\n",
        "plot_residuals(y_test, model_metrics['R¬≤ (Coefficient of Determination)'])  # Corrected key here\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ R¬≤ (Coefficient of Determination)\n",
        "# ------------------------------------------\n",
        "def calculate_r2(model, X_test, y_test):\n",
        "    \"\"\"Calculate R¬≤ (Coefficient of Determination)\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"üßÆ R¬≤ (Coefficient of Determination): {r2:.4f}\")\n",
        "    return r2\n",
        "\n",
        "# Calculate R¬≤\n",
        "r2 = calculate_r2(lr, X_test, y_test)\n",
        "\n",
        "# Final insights\n",
        "print(\"\\nüöÄ House Price Prediction Model Summary:\")\n",
        "print(f\"Best Features Impacting House Price: {list(coef_df.head(3)['Feature'])}\")\n",
        "print(f\"Model R¬≤: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY63hj8nCCFR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlic3C5HCDXX"
      },
      "source": [
        "# üìä House Price Prediction ‚Äì Model Results\n",
        "\n",
        "## üìä 1. Model Performance Summary\n",
        "\n",
        "| Metric                        | Value       | Interpretation                                                             |\n",
        "|-------------------------------|-------------|-----------------------------------------------------------------------------|\n",
        "| **Mean Absolute Error (MAE)**  | 50,701.78   | The average absolute difference between predicted and actual house prices.  |\n",
        "| **Mean Squared Error (MSE)**   | 4,904,399,775.95 | The average squared difference, more sensitive to large errors.            |\n",
        "| **Root Mean Squared Error (RMSE)** | 70,031.42  | The square root of MSE, indicating the typical error in house price prediction. |\n",
        "| **R¬≤ (Coefficient of Determination)** | 0.6257   | 62.57% of the variance in house prices is explained by the model.            |\n",
        "| **Explained Variance**         | 0.6258      | The proportion of variance explained by the model.                          |\n",
        "\n",
        "## üîç 2. Most Influential Features for House Price Prediction\n",
        "\n",
        "### üö® Top Features Impacting House Price:\n",
        "\n",
        "| Feature                    | Coefficient   | Interpretation                                             |\n",
        "|----------------------------|---------------|------------------------------------------------------------|\n",
        "| **ocean_proximity_ISLAND**  | +136,125.07   | Significantly increases house price compared to other locations. |\n",
        "| **median_income**           | +86,047.34    | Higher income areas contribute to higher house prices.       |\n",
        "| **total_bedrooms**          | +35,590.83    | More bedrooms typically correlate with higher house prices. |\n",
        "| **households**              | +35,005.78    | Larger households tend to be in areas with higher house prices. |\n",
        "| **housing_median_age**      | +20,941.52    | Older homes tend to be in more valuable areas.              |\n",
        "| **ocean_proximity_NEAR OCEAN** | +3,431.14   | Proximity to the ocean increases house value.                |\n",
        "| **ocean_proximity_NEAR BAY** | -5,136.64    | Houses near bays tend to have a lower price compared to oceanfront properties. |\n",
        "| **total_rooms**             | -10,238.07     | An inverse relationship between the total number of rooms and price, possibly due to house type or location. |\n",
        "| **population**              | -35,806.19    | Larger populations might indicate lower-value areas.         |\n",
        "| **ocean_proximity_INLAND**  | -39,719.68    | Inland areas generally have lower house prices.               |\n",
        "\n",
        "## üéØ Performance Analysis\n",
        "\n",
        "### üî¨ Detailed Insights:\n",
        "- **R¬≤ (0.6257)**: The model explains approximately **62.57%** of the variance in house prices, which is a decent result for linear regression, especially with complex real-world data.\n",
        "- **High RMSE**: The root mean squared error (RMSE) is relatively high, indicating that the model has significant prediction errors for some houses.\n",
        "- **Moderate MAE**: The mean absolute error (MAE) suggests that the model is, on average, off by about **$50,701.78**.\n",
        "\n",
        "### üöß Challenges:\n",
        "1. **Model Performance**:\n",
        "   - While the R¬≤ is fairly decent, the **high RMSE** and **MSE** indicate that there are substantial errors in predicting certain house prices.\n",
        "   \n",
        "2. **Feature Impact**:\n",
        "   - **ocean_proximity_ISLAND** and **median_income** have the most substantial positive coefficients, meaning these features significantly influence the house prices.\n",
        "\n",
        "3. **Negative Coefficients**:\n",
        "   - Features like **ocean_proximity_NEAR BAY** and **ocean_proximity_INLAND** have negative coefficients, suggesting that houses in these areas tend to have lower prices.\n",
        "\n",
        "### üí° Practical Recommendations:\n",
        "\n",
        "#### üõ°Ô∏è Model Improvement Strategies:\n",
        "1. **Feature Engineering**:\n",
        "   - Investigate additional features that may improve the model's performance (e.g., incorporating data like nearby amenities, economic factors, etc.).\n",
        "   \n",
        "2. **Non-Linear Models**:\n",
        "   - Try using **non-linear models** like **Random Forest** or **Gradient Boosting** to capture more complex relationships in the data.\n",
        "\n",
        "3. **Outlier Handling**:\n",
        "   - Consider handling **outliers** more effectively (e.g., through **log transformation** or using more robust models).\n",
        "\n",
        "4. **Interaction Features**:\n",
        "   - Investigate potential interactions between features that could better explain the variability in house prices.\n",
        "\n",
        "### üè¶ Operational Considerations:\n",
        "- **Model Usage**: This model works well in predicting **general trends** but might need refinement to handle rare cases or specific price ranges with better precision.\n",
        "- **Feature Importance**: Based on the coefficients, **ocean proximity** and **median income** are crucial in determining house price predictions.\n",
        "\n",
        "## üìå Key Takeaways:\n",
        "- The model performs decently in predicting house prices, with **62.57% of variance explained**.\n",
        "- **Proximity to the ocean** and **income levels** are the most influential features.\n",
        "- The model's performance can be improved by addressing outliers, using more sophisticated models, and adding engineered features.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ This analysis provides insights into the factors influencing house prices and highlights the model's performance, along with areas where improvements could be made.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7Bt-3liCTaH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Feature Coefficient Visualization\n",
        "def plot_top_coefficients(coef_df, N=10):\n",
        "    \"\"\"\n",
        "    Visualize top N most influential features by their coefficients\n",
        "\n",
        "    Parameters:\n",
        "    - coef_df: DataFrame with 'Feature', 'Coefficient' columns\n",
        "    - N: Number of top features to display\n",
        "    \"\"\"\n",
        "    # Compute absolute value of coefficients\n",
        "    coef_df['|Coefficient|'] = np.abs(coef_df['Coefficient'])\n",
        "\n",
        "    # Sort by magnitude and select top N\n",
        "    top_coef_df = coef_df.sort_values(by='|Coefficient|', ascending=False).head(N)\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Color coding: red for positive, blue for negative coefficients\n",
        "    colors = ['red' if c > 0 else 'blue' for c in top_coef_df['Coefficient']]\n",
        "\n",
        "    # Horizontal bar plot\n",
        "    bars = plt.barh(top_coef_df['Feature'], top_coef_df['Coefficient'], color=colors)\n",
        "\n",
        "    # Styling\n",
        "    plt.xlabel(\"Coefficient Value (Impact on House Price)\", fontsize=12)\n",
        "    plt.title(f\"Top {N} Most Influential Features in House Price Prediction\", fontsize=14)\n",
        "    plt.axvline(0, color='gray', linestyle='--')\n",
        "    plt.gca().invert_yaxis()  # Highest at top\n",
        "    plt.grid(True, axis='x')\n",
        "\n",
        "    # Add coefficient values as text\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
        "                 f'{width:.3f}',\n",
        "                 ha='left' if width > 0 else 'right',\n",
        "                 va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming coef_df is a DataFrame with 'Feature' and 'Coefficient' columns from your linear regression model\n",
        "# Example: coef_df for house price prediction\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': ['ocean_proximity_ISLAND', 'median_income', 'total_bedrooms', 'households', 'housing_median_age',\n",
        "                'ocean_proximity_NEAR OCEAN', 'ocean_proximity_NEAR BAY', 'total_rooms', 'population', 'ocean_proximity_INLAND'],\n",
        "    'Coefficient': [156041.97, 84523.50, 25030.27, 25005.78, 20099.41, 4747.40, -3689.75, -8114.49, -36827.61, -39719.68]\n",
        "})\n",
        "\n",
        "# Choose number of top features to display\n",
        "N = 10\n",
        "\n",
        "# Plot top features\n",
        "plot_top_coefficients(coef_df, N)\n",
        "\n",
        "# Additional Insights\n",
        "print(\"\\nüîç Top Features Interpretation:\")\n",
        "top_features = coef_df.sort_values(by='|Coefficient|', ascending=False).head(N)\n",
        "for _, row in top_features.iterrows():\n",
        "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
        "    print(f\"- {row['Feature']} {direction} house price by {abs(row['Coefficient']):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfMb4SotCYnY"
      },
      "source": [
        "üß† Interpretation:\n",
        "Red bars: Features with positive coefficients. A positive coefficient means that as the value of the feature increases, the predicted outcome (e.g., house price or churn risk) increases.\n",
        "Blue bars: Features with negative coefficients. A negative coefficient means that as the value of the feature increases, the predicted outcome decreases.\n",
        "The further from 0, the stronger the influence of the feature on the predicted outcome. Features with coefficients farther from 0 have a larger impact on the model's predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKE4qOYnCsTT"
      },
      "source": [
        "Here‚Äôs the updated **text cell** for your **Basic Neural Network Model for Regression**, using LaTeX syntax with `$$` for equations and `$` for formulas:\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Basic Neural Network Model for Regression\n",
        "\n",
        "## 1Ô∏è‚É£ Why Neural Networks?\n",
        "\n",
        "While **linear regression** is a **linear model**, neural networks can learn **nonlinear relationships** between features and the target variable. This allows them to capture **more complex patterns** and interactions that might not be detected by a linear model.\n",
        "\n",
        "We now construct a **feedforward neural network** (also called a **multilayer perceptron**) with:\n",
        "\n",
        "- One **hidden layer** with a **nonlinear activation function**\n",
        "- One **output neuron** with a **linear activation function** (for regression)\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Model Architecture\n",
        "\n",
        "The architecture of the neural network for regression is as follows:\n",
        "\n",
        "- **Input Layer**:\n",
        "  - The input layer receives the feature vector $\\mathbf{x} \\in \\mathbb{R}^d$, where $d$ is the number of features.\n",
        "\n",
        "- **Hidden Layer**:\n",
        "  - **Linear Transformation**:\n",
        "    $$\n",
        "    \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\n",
        "    $$\n",
        "    where $\\mathbf{W}^{(1)}$ is the weight matrix and $\\mathbf{b}^{(1)}$ is the bias vector.\n",
        "  - **Nonlinearity**:\n",
        "    $$\n",
        "    \\mathbf{a}^{(1)} = \\text{ReLU}(\\mathbf{z}^{(1)})\n",
        "    $$\n",
        "    where ReLU (Rectified Linear Unit) introduces nonlinearity by transforming all negative values to zero while leaving positive values unchanged.\n",
        "\n",
        "- **Output Layer**:\n",
        "  - **Linear Transformation**:\n",
        "    $$\n",
        "    \\mathbf{z}^{(2)} = \\mathbf{W}^{(2)} \\cdot \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}\n",
        "    $$\n",
        "  - **Linear Activation**:\n",
        "    $$\n",
        "    \\hat{y} = \\mathbf{z}^{(2)}\n",
        "    $$\n",
        "    where $\\hat{y}$ is the predicted output, such as the predicted house price, which can take any real value.\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Activation Functions\n",
        "\n",
        "- **ReLU (Rectified Linear Unit)** in the hidden layer:\n",
        "\n",
        "  $$\n",
        "  \\text{ReLU}(z) = \\max(0, z)\n",
        "  $$\n",
        "\n",
        "  - ReLU introduces **nonlinearity** by setting all negative values to zero and keeping positive values unchanged. This helps the network learn more complex patterns.\n",
        "\n",
        "- **Linear Activation** in the output layer:\n",
        "  - For regression tasks, we use a **linear activation** in the output layer to allow the model to predict continuous values (e.g., house prices).\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Loss Function\n",
        "\n",
        "For regression, we use the **Mean Squared Error (MSE)** loss function:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "$$\n",
        "\n",
        "- MSE measures the average squared difference between the predicted values ($\\hat{y}$) and the true values ($y$).\n",
        "- The goal of the neural network is to **minimize this loss**, making the predicted values as close as possible to the true values.\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Optimization\n",
        "\n",
        "The model parameters (weights and biases) are learned using **stochastic gradient descent (SGD)** or more advanced optimizers like **Adam**, which adjust the learning rate during training to speed up convergence and improve model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## üÜö Linear Regression vs Neural Network\n",
        "\n",
        "| Property            | Linear Regression            | Neural Network (1 hidden layer)       |\n",
        "|---------------------|------------------------------|---------------------------------------|\n",
        "| **Linearity**        | Linear relationship           | Nonlinear, flexible decision boundary |\n",
        "| **Interpretability** | High (coefficients are easy to interpret) | Lower (harder to explain due to complexity) |\n",
        "| **Expressive Power** | Limited (only linear relationships) | Higher (can capture complex patterns and interactions) |\n",
        "| **Training Time**    | Fast                         | Slower (due to complexity and more parameters) |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ In the next step, we will implement this **basic neural network** using **TensorFlow/Keras** and compare its performance to **linear regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Updates for **Regression**:\n",
        "1. **Linear Activation in the Output Layer**:\n",
        "   - For regression tasks, the output layer has a **linear activation** instead of a **sigmoid**. This allows the model to output continuous values.\n",
        "\n",
        "2. **Mean Squared Error (MSE)**:\n",
        "   - **MSE** is used as the loss function for regression instead of **binary cross-entropy**, which is used in classification.\n",
        "\n",
        "3. **Neural Network Power**:\n",
        "   - Neural networks can learn **nonlinear relationships** between the input features and the output, making them more powerful than simple linear regression when dealing with complex data.\n",
        "\n",
        "### Example Use Case:\n",
        "- **House Price Prediction**: Using a neural network with a hidden layer and ReLU activation can help capture the **complex relationships** between features like location, size, and age of the house, which a simple linear regression model might miss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7uYQl5tCx5T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define z range\n",
        "z = np.linspace(-10, 10, 400)\n",
        "\n",
        "# Define activation functions\n",
        "sigmoid = 1 / (1 + np.exp(-z))\n",
        "relu = np.maximum(0, z)\n",
        "tanh = np.tanh(z)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, relu, label=\"ReLU\", linewidth=2)\n",
        "plt.plot(z, sigmoid, label=\"Sigmoid\", linewidth=2)\n",
        "plt.plot(z, tanh, label=\"Tanh\", linewidth=2)\n",
        "plt.title(\"Activation Functions\")\n",
        "plt.xlabel(\"Input (z)\")\n",
        "plt.ylabel(\"Activation Output\")\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7mHrZvJC4G-"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "from IPython.display import Image\n",
        "\n",
        "# Create the neural network diagram\n",
        "dot = Digraph(format='png')\n",
        "dot.attr(rankdir='LR', size='10,6')\n",
        "dot.attr('node', shape='circle', style='filled', fontname='Helvetica')\n",
        "\n",
        "# Define layers\n",
        "input_features = ['x1', 'x2', 'x3']\n",
        "hidden_neurons = ['h1', 'h2', 'h3']\n",
        "output_neuron = '≈∑'\n",
        "\n",
        "# Input layer\n",
        "with dot.subgraph(name='cluster_input') as c:\n",
        "    c.attr(label='Input Layer', color='lightgray')\n",
        "    for f in input_features:\n",
        "        c.node(f, label=f, fillcolor='lightblue')\n",
        "\n",
        "# Hidden layer\n",
        "with dot.subgraph(name='cluster_hidden') as c:\n",
        "    c.attr(label='Hidden Layer (ReLU)', color='lightgray')\n",
        "    for h in hidden_neurons:\n",
        "        c.node(h, label=h, fillcolor='lightgreen')\n",
        "\n",
        "# Output neuron for regression\n",
        "dot.node(output_neuron, label='≈∑\\n(Linear)', fillcolor='orange')\n",
        "\n",
        "# Connect input to hidden with indexed weights\n",
        "for i, f in enumerate(input_features):\n",
        "    for j, h in enumerate(hidden_neurons):\n",
        "        dot.edge(f, h, label=f'w{i+1}{j+1}')\n",
        "\n",
        "# Connect hidden to output with indexed weights\n",
        "for j, h in enumerate(hidden_neurons):\n",
        "    dot.edge(h, output_neuron, label=f'w{j+1}o')\n",
        "\n",
        "# Add output annotation as plain text (safe and portable)\n",
        "dot.node('note', label='≈∑ (Continuous)\\nPredicted Value', shape='note', fontname='Helvetica', color='gold')\n",
        "dot.edge(output_neuron, 'note', style='dashed')\n",
        "\n",
        "# Render\n",
        "dot.render('simple_nn_architecture_regression', cleanup=False)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image('simple_nn_architecture_regression.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvzFcjVyqkfe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGRk2X2aMKo5"
      },
      "source": [
        "\n",
        "\n",
        "# üìò Neural Network Forward Pass Equations\n",
        "\n",
        "The feedforward neural network in the diagram has:\n",
        "\n",
        "- 3 input features: $x_1, x_2, x_3$\n",
        "- 3 hidden units: $h_1, h_2, h_3$\n",
        "- 1 output unit: $\\hat{y}$\n",
        "- ReLU activation in the hidden layer\n",
        "- **Linear activation** in the output layer for regression\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Step 1: Hidden Layer Computation\n",
        "\n",
        "Each hidden neuron computes a weighted sum of the inputs and applies the ReLU activation:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "h_1 &= \\text{ReLU}(w_{11} x_1 + w_{21} x_2 + w_{31} x_3 + b_1) \\\\\n",
        "h_2 &= \\text{ReLU}(w_{12} x_1 + w_{22} x_2 + w_{32} x_3 + b_2) \\\\\n",
        "h_3 &= \\text{ReLU}(w_{13} x_1 + w_{23} x_2 + w_{33} x_3 + b_3)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "- $w_{ij}$: weight from input $x_i$ to hidden neuron $h_j$\n",
        "- $b_j$: bias term for hidden neuron $h_j$\n",
        "- ReLU is defined as:  \n",
        "  $$\\text{ReLU}(z) = \\max(0, z)$$\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Step 2: Output Layer Computation\n",
        "\n",
        "The output is computed using a weighted sum of the hidden activations, without applying an activation function (linear):\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_{1o} h_1 + w_{2o} h_2 + w_{3o} h_3 + b_0\n",
        "$$\n",
        "\n",
        "- $w_{jo}$: weight from hidden neuron $h_j$ to the output neuron  \n",
        "- $b_0$: output layer bias\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Output Interpretation\n",
        "\n",
        "- $\\hat{y}$ is the **predicted continuous value**, such as a predicted house price.\n",
        "- The model outputs a **real-valued prediction**, and there is no threshold applied (as in classification).\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ These equations define the **forward computation** of a 1-hidden-layer neural network used for **regression**, where the goal is to predict a continuous value rather than a binary classification outcome.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5ALWezRMmUz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# üìò Neural Network: Backpropagation, Initialization, and Training Concepts\n",
        "\n",
        "Once the forward pass computes the prediction $\\hat{y}$, the network learns by **minimizing the mean squared error (MSE) loss** using **gradient-based optimization**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Weight Initialization\n",
        "\n",
        "Weight initialization affects **how quickly** and **how well** the network trains.\n",
        "\n",
        "- If all weights are initialized to the same value (e.g., 0), all neurons compute the same output ‚Äî this breaks learning due to **symmetry**.\n",
        "- Instead, weights are initialized randomly (but carefully) to break symmetry while controlling variance.\n",
        "\n",
        "Common initialization methods:\n",
        "\n",
        "- **Xavier (Glorot) Initialization**: used for sigmoid/tanh activations  \n",
        "  $$\n",
        "  W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}} \\right)\n",
        "  $$\n",
        "\n",
        "- **He Initialization**: used for ReLU activation  \n",
        "  $$\n",
        "  W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}} \\right)\n",
        "  $$\n",
        "\n",
        "Biases are usually initialized to **zero** or a small constant.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Loss Function\n",
        "\n",
        "For a single training example $(\\mathbf{x}, y)$, where $y$ is a **continuous target value**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\hat{y}, y) = \\frac{1}{2} \\left( y - \\hat{y} \\right)^2\n",
        "$$\n",
        "\n",
        "This is the **mean squared error (MSE)** loss function, which penalizes the squared difference between the predicted value $\\hat{y}$ and the true value $y$. It is the most commonly used loss function for regression tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Backpropagation: Gradient Computation\n",
        "\n",
        "The **goal** is to compute the gradient of the loss $\\mathcal{L}$ with respect to all model parameters ‚Äî these gradients are used to update weights during training.\n",
        "\n",
        "### üîπ Output Layer\n",
        "\n",
        "Let:\n",
        "\n",
        "$$\n",
        "z_o = w_{1o} h_1 + w_{2o} h_2 + w_{3o} h_3 + b_0, \\quad \\hat{y} = z_o\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_o} = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "And using the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{jo}} = (\\hat{y} - y) \\cdot h_j, \\quad\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b_0} = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Hidden Layer\n",
        "\n",
        "For each hidden unit $j$:\n",
        "\n",
        "$$\n",
        "z_j = \\sum_i w_{ij} x_i + b_j, \\quad h_j = \\text{ReLU}(z_j)\n",
        "$$\n",
        "\n",
        "The derivative of ReLU is:\n",
        "\n",
        "$$\n",
        "\\frac{d}{dz_j} \\text{ReLU}(z_j) =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } z_j > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Backpropagating the error:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_j} = (\\hat{y} - y) \\cdot w_{jo} \\cdot \\mathbb{1}_{\\{z_j > 0\\}}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} = \\frac{\\partial \\mathcal{L}}{\\partial z_j} \\cdot x_i, \\quad\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b_j} = \\frac{\\partial \\mathcal{L}}{\\partial z_j}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Gradient Descent Step\n",
        "\n",
        "Each parameter $\\theta$ is updated using the **learning rate** $\\eta$:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
        "$$\n",
        "\n",
        "### üîπ What is $\\eta$?\n",
        "\n",
        "- **$\\eta$ (eta)** is the **learning rate**, a small positive scalar that controls how big each update step is.\n",
        "- If $\\eta$ is **too large**, training may diverge. If **too small**, convergence is slow.\n",
        "\n",
        "**Typical values**: $0.1$, $0.01$, $0.001$  \n",
        "Use **adaptive optimizers** (e.g., Adam) to handle learning rate dynamically.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Key Training Concepts\n",
        "\n",
        "### üîπ Epoch\n",
        "\n",
        "An **epoch** is one full pass through the entire training dataset.  \n",
        "Typically, models are trained for **multiple epochs** (e.g., 10‚Äì100) to improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Mini-Batch Gradient Descent\n",
        "\n",
        "Instead of computing gradients on the full dataset (which is slow), we use **mini-batches** (e.g., 32‚Äì128 samples) to compute approximate gradients:\n",
        "\n",
        "- **More updates per epoch**.\n",
        "- Helps generalization due to added noise.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Early Stopping\n",
        "\n",
        "A regularization strategy to prevent **overfitting**:\n",
        "\n",
        "- Monitor validation loss during training.\n",
        "- Stop training when validation performance **no longer improves** for several epochs.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Cross-Validation\n",
        "\n",
        "Used to evaluate model **generalization** and tune **hyperparameters**:\n",
        "\n",
        "- Split training data into $K$ subsets (folds).\n",
        "- Train on $K-1$ folds and validate on the remaining one.\n",
        "- Repeat for all folds.\n",
        "\n",
        "This gives more **robust performance estimates** than a single train/test split.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ These components form the **backbone of neural network training**: from initialization and forward computation to backpropagation, optimization, and generalization control.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ3wvBSXOOtn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Simple convex function: f(w1, w2) = w1^2 + w2^2\n",
        "def loss(w1, w2):\n",
        "    return w1**2 + w2**2\n",
        "\n",
        "# Grid for 3D surface\n",
        "w1 = np.linspace(-2, 2, 100)\n",
        "w2 = np.linspace(-2, 2, 100)\n",
        "W1, W2 = np.meshgrid(w1, w2)\n",
        "L = loss(W1, W2)\n",
        "\n",
        "# Simulate gradient descent path\n",
        "lr = 0.1\n",
        "steps = 20\n",
        "path = [(1.8, -1.5)]  # initial point\n",
        "\n",
        "for _ in range(steps):\n",
        "    w1, w2 = path[-1]\n",
        "    grad = (2*w1, 2*w2)  # gradient of w1^2 + w2^2\n",
        "    new_w1 = w1 - lr * grad[0]\n",
        "    new_w2 = w2 - lr * grad[1]\n",
        "    path.append((new_w1, new_w2))\n",
        "\n",
        "path = np.array(path)\n",
        "path_loss = loss(path[:, 0], path[:, 1])\n",
        "\n",
        "# Plot surface\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(W1, W2, L, cmap='viridis', alpha=0.6)\n",
        "ax.plot(path[:, 0], path[:, 1], path_loss, color='red', marker='o', label='Gradient Descent Path')\n",
        "\n",
        "ax.set_xlabel('w1')\n",
        "ax.set_ylabel('w2')\n",
        "ax.set_zlabel('Loss')\n",
        "ax.set_title('3D Loss Surface with Gradient Descent')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzZKwLsFOPFl"
      },
      "source": [
        "# üìò Guidelines for Choosing Neural Network Hyperparameters\n",
        "\n",
        "Training a neural network involves selecting a set of **hyperparameters** that significantly impact learning dynamics and model performance. This section defines and explains key hyperparameters, including how they work and how to choose them effectively.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß 1. Learning Rate ($\\eta$)\n",
        "\n",
        "The **learning rate** $\\eta$ controls how much the model updates its weights during each step of gradient descent:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}\n",
        "$$\n",
        "\n",
        "- **Too small**: learning is slow and may converge to a suboptimal minimum.\n",
        "- **Too large**: learning is unstable and may diverge.\n",
        "\n",
        "**Recommended values**: $10^{-2}$ to $10^{-4}$  \n",
        "**Best practice**:\n",
        "- Start with $\\eta = 0.001$ when using Adam\n",
        "- Use **learning rate scheduling** (e.g., reduce on plateau)\n",
        "- Prefer **adaptive optimizers** (e.g., Adam, RMSprop)\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ 2. Number of Epochs\n",
        "\n",
        "An **epoch** is one full pass through the entire training set.\n",
        "\n",
        "- **Under-training** (too few epochs): model fails to capture patterns.\n",
        "- **Overfitting** (too many epochs): model memorizes training data and generalizes poorly.\n",
        "\n",
        "**Tip**: Monitor **training and validation loss** and apply **early stopping** when validation loss stops improving for a fixed number of epochs (e.g., 5‚Äì10).\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ 3. Hidden Layers and Units\n",
        "\n",
        "The **depth and width** of the network define its **capacity** to learn complex functions.\n",
        "\n",
        "- **Too shallow or narrow**: high bias (underfitting)\n",
        "- **Too deep or wide**: high variance (overfitting)\n",
        "\n",
        "**Guidelines**:\n",
        "- Start with 1‚Äì2 hidden layers\n",
        "- Try 8, 16, 32, or 64 neurons per layer\n",
        "- Adjust based on validation performance\n",
        "\n",
        "Use **cross-validation** to compare architectures.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ 4. Batch Size\n",
        "\n",
        "**Batch size** is the number of samples used to compute the gradient in one update step.\n",
        "\n",
        "| Batch Size | Characteristics               | Trade-offs                       |\n",
        "|------------|-------------------------------|----------------------------------|\n",
        "| Small (e.g., 32) | More noisy updates, faster convergence | Less stable, good generalization |\n",
        "| Large (e.g., 512) | Smoother gradients, slower updates    | May overfit, high memory usage   |\n",
        "\n",
        "**Typical values**: 32, 64, 128  \n",
        "**Tip**: Use powers of 2 for efficiency on GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ 5. Activation Functions\n",
        "\n",
        "Activations introduce **nonlinearity**, enabling the network to model complex patterns.\n",
        "\n",
        "### Common functions:\n",
        "\n",
        "- **ReLU** (default for hidden layers):  \n",
        "  $$\\text{ReLU}(z) = \\max(0, z)$$  \n",
        "  Efficient and sparsity-inducing.\n",
        "\n",
        "- **Sigmoid** (used in output layer for binary classification):  \n",
        "  $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$  \n",
        "  Maps to $(0, 1)$.\n",
        "\n",
        "- **Tanh**:  \n",
        "  $$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$  \n",
        "  Zero-centered, often better than sigmoid in hidden layers.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ° 6. Regularization Techniques\n",
        "\n",
        "Regularization helps prevent **overfitting** by constraining model complexity.\n",
        "\n",
        "### üîπ L2 Regularization (Weight Decay)\n",
        "\n",
        "Adds a penalty to the loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda \\sum w_i^2\n",
        "$$\n",
        "\n",
        "- Penalizes large weights\n",
        "- Encourages simpler models\n",
        "\n",
        "### üîπ Dropout\n",
        "\n",
        "Randomly disables a fraction $p$ of neurons during training:\n",
        "\n",
        "- Prevents **co-adaptation** of units\n",
        "- Helps generalization\n",
        "\n",
        "**Typical dropout rates**: $p = 0.2$ to $0.5$\n",
        "\n",
        "At inference, all neurons are active and weights are scaled accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è 7. Optimizers\n",
        "\n",
        "Algorithms used to minimize the loss by updating weights.\n",
        "\n",
        "### üîπ SGD (Stochastic Gradient Descent)\n",
        "\n",
        "- Simple, interpretable\n",
        "- Sensitive to learning rate and scale\n",
        "\n",
        "### üîπ Adam\n",
        "\n",
        "- Adaptive learning rates per parameter\n",
        "- Combines momentum and RMSprop\n",
        "- **Recommended default** for most problems\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 8. Hyperparameter Tuning\n",
        "\n",
        "Explore combinations using:\n",
        "\n",
        "- **Grid Search**: test all value combinations\n",
        "- **Random Search**: sample combinations randomly\n",
        "- **Bayesian Optimization**: learn which settings work best\n",
        "\n",
        "Always use a **validation set** or **cross-validation** to evaluate tuning performance.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ These hyperparameters govern how the model learns and generalizes. Choosing them wisely is key to building robust and performant neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lPxXNbPTvXc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1Ô∏è‚É£ Neural Network Model Construction for Regression\n",
        "def create_neural_network(input_shape):\n",
        "    \"\"\"\n",
        "    Create a basic neural network for regression\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: Shape of input features\n",
        "\n",
        "    Returns:\n",
        "    - Compiled TensorFlow/Keras model\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # Input layer with dropout for regularization\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,),\n",
        "              kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Hidden layer\n",
        "        Dense(32, activation='relu',\n",
        "              kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Output layer with linear activation for regression\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mean_squared_error',\n",
        "        metrics=['mean_absolute_error', 'mean_squared_error']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# 2Ô∏è‚É£ Training and Evaluation Function for Regression\n",
        "def train_and_evaluate_nn(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train neural network for regression and provide comprehensive evaluation\n",
        "\n",
        "    Parameters:\n",
        "    - X_train, X_test: Feature matrices\n",
        "    - y_train, y_test: Target vectors\n",
        "\n",
        "    Returns:\n",
        "    - Trained model\n",
        "    - Training history\n",
        "    - Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_neural_network(X_train.shape[1])\n",
        "\n",
        "    # Training\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    metrics = {\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'R¬≤': r2_score(y_test, y_pred),\n",
        "        'MAE': mean_absolute_error(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Detailed Evaluation\n",
        "    print(\"üö® Neural Network Performance (Regression):\")\n",
        "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "    print(f\"R¬≤: {metrics['R¬≤']:.4f}\")\n",
        "    print(f\"MAE: {metrics['MAE']:.4f}\")\n",
        "\n",
        "    # Learning Curves\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
        "    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "    plt.title('Model MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, history, metrics\n",
        "\n",
        "# 3Ô∏è‚É£ Model Training and Evaluation\n",
        "# Use preprocessed data from previous steps\n",
        "nn_model, nn_history, nn_metrics = train_and_evaluate_nn(\n",
        "    X_train, X_test, y_train, y_test  #\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£ Comparative Analysis\n",
        "print(\"\\nüîç Neural Network vs Linear Regression:\")\n",
        "# Optionally compare with linear regression metrics if needed\n",
        "print(f\"Neural Network R¬≤: {nn_metrics['R¬≤']:.4f}\")\n",
        "# Example for linear regression model comparison\n",
        "print(f\"Linear Regression R¬≤: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1yrEVurqspe"
      },
      "source": [
        "#Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzKwCSJrqxMv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# üö® Neural Network for House Price Prediction\n",
        "\n",
        "## 1Ô∏è‚É£ Why Neural Networks for House Price Prediction?\n",
        "While **linear regression** provides a simple linear relationship, **neural networks** can capture **nonlinear patterns** in house price data, which may involve complex interactions between various features such as location, number of rooms, and household characteristics.\n",
        "\n",
        "Neural networks offer:\n",
        "- The ability to learn sophisticated feature interactions.\n",
        "- Flexibility in modeling complex relationships, such as non-linearities between variables.\n",
        "- Adaptability to high-dimensional and large datasets, making them suitable for more complex real estate predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Model Architecture\n",
        "Our **neural network architecture** for house price prediction consists of several layers:\n",
        "- **Input Layer**: Receives multiple features, such as square footage, number of rooms, and neighborhood data.\n",
        "- **First Hidden Layer**: 64 neurons with **ReLU** activation to introduce nonlinearity.\n",
        "- **Second Hidden Layer**: 32 neurons with **ReLU** activation to further capture interactions.\n",
        "- **Third Hidden Layer**: 16 neurons with **ReLU** activation to refine the model's representation.\n",
        "- **Output Layer**: A single neuron with **linear activation** to predict house price.\n",
        "\n",
        "### Mathematical Representation:\n",
        "- **Input Layer**: $\\mathbf{x} \\in \\mathbb{R}^{d}$ where $d$ is the number of features\n",
        "- **First Hidden Layer**:\n",
        "  $$\n",
        "  \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\n",
        "  $$\n",
        "  $$\n",
        "  \\mathbf{a}^{(1)} = \\text{ReLU}(\\mathbf{z}^{(1)})\n",
        "  $$\n",
        "- **Output Layer**:\n",
        "  $$\n",
        "  \\hat{y} = \\mathbf{w}^{(2)} \\cdot \\mathbf{a}^{(1)} + b^{(2)}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Activation Functions\n",
        "- **ReLU (Rectified Linear Unit)** in hidden layers:\n",
        "  $$\n",
        "  \\text{ReLU}(z) = \\max(0, z)\n",
        "  $$\n",
        "  **Advantages**:\n",
        "  - Introduces nonlinearity into the network, allowing it to model more complex patterns.\n",
        "  - Mitigates the vanishing gradient problem, which helps the model learn effectively during training.\n",
        "  - Computationally efficient, speeding up the training process.\n",
        "\n",
        "- **Linear Activation** in the output layer:\n",
        "  - The output is a continuous value, representing the predicted house price.\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Regularization Techniques\n",
        "\n",
        "### üõ°Ô∏è L2 Regularization\n",
        "Prevents overfitting by penalizing large weights:\n",
        "$$\n",
        "\\text{Regularization Loss} = \\lambda \\sum_{l} \\|\\mathbf{W}^{(l)}\\|_2^2\n",
        "$$\n",
        "Where $\\lambda = 0.001$ is the regularization coefficient applied to each layer‚Äôs weight matrix.\n",
        "\n",
        "### üé≤ Dropout Regularization\n",
        "To prevent overfitting, we apply **dropout** to randomly deactivate neurons during training:\n",
        "- **First layer**: 30% dropout\n",
        "- **Helps improve generalization** by reducing the reliance on any specific feature.\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Loss Function\n",
        "For regression tasks, we use **Mean Squared Error (MSE)**:\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "This loss function is ideal for predicting continuous values, such as house prices, and penalizes large errors more significantly.\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Optimization Strategy\n",
        "- **Adam Optimizer**:\n",
        "  - Adaptive learning rate helps the model converge faster.\n",
        "  - Momentum-based gradient descent leads to efficient optimization.\n",
        "  - Learning rate: 0.001.\n",
        "\n",
        "- **Early Stopping**:\n",
        "  - Stops training when the validation loss stops improving, preventing overfitting and saving computation time.\n",
        "\n",
        "---\n",
        "\n",
        "## üÜö Linear Regression vs Deep Neural Network\n",
        "\n",
        "| Property                | Linear Regression        | Deep Neural Network         |\n",
        "|-------------------------|--------------------------|-----------------------------|\n",
        "| Decision Boundary       | Linear                   | Highly Nonlinear            |\n",
        "| Feature Interaction     | Limited                  | Complex, Multilevel         |\n",
        "| House Price Prediction  | Basic                    | Sophisticated, Adaptive     |\n",
        "| Computational Complexity| Low                      | Higher                      |\n",
        "| Interpretability        | High (Coefficients)      | Lower (Harder to Explain)   |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Advantages for House Price Prediction\n",
        "- **Captures nonlinear relationships** between features and house price.\n",
        "- **Adapts to complex data** like neighborhood characteristics, building age, and house size.\n",
        "- **Handles high-dimensional data** effectively, making it robust for real estate prediction tasks.\n",
        "\n",
        "‚úÖ Neural networks are a powerful approach to predicting house prices, capable of learning intricate patterns from the dataset to provide accurate price predictions.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oOumj30qyhK"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_house_price_nn_diagram():\n",
        "    \"\"\"\n",
        "    Create a detailed visualization of the neural network\n",
        "    for house price prediction.\n",
        "    \"\"\"\n",
        "    dot = graphviz.Digraph(format='png', engine='dot')\n",
        "    dot.attr(rankdir='LR', size='16,10')\n",
        "    dot.attr('node', shape='circle', style='filled', fontname='Helvetica')\n",
        "\n",
        "    # Color palette\n",
        "    colors = {\n",
        "        'input': 'lightblue',\n",
        "        'hidden1': 'lightgreen',\n",
        "        'hidden2': 'lightseagreen',\n",
        "        'hidden3': 'mediumaquamarine',\n",
        "        'output': 'orange'\n",
        "    }\n",
        "\n",
        "    # Input Layer\n",
        "    with dot.subgraph(name='cluster_input') as c:\n",
        "        c.attr(label='Input Layer\\n(Features: Area, Bedrooms, etc.)', color='gray')\n",
        "        for i in range(5):  # Show first 5 features\n",
        "            c.node(f'x{i+1}', label=f'Feature-{i+1}', fillcolor=colors['input'])\n",
        "        c.node('x_etc', label='...', style='filled', fillcolor='white')\n",
        "\n",
        "    # First Hidden Layer (64 neurons)\n",
        "    with dot.subgraph(name='cluster_hidden1') as c:\n",
        "        c.attr(label='First Hidden Layer\\n(64 neurons, ReLU)', color='gray')\n",
        "        for i in range(5):  # Show first 5 neurons\n",
        "            c.node(f'h1_{i}', label=f'H1-{i+1}', fillcolor=colors['hidden1'])\n",
        "        c.node('h1_etc', label='...', style='filled', fillcolor='white')\n",
        "\n",
        "    # Second Hidden Layer (32 neurons)\n",
        "    with dot.subgraph(name='cluster_hidden2') as c:\n",
        "        c.attr(label='Second Hidden Layer\\n(32 neurons, ReLU)', color='gray')\n",
        "        for i in range(5):  # Show first 5 neurons\n",
        "            c.node(f'h2_{i}', label=f'H2-{i+1}', fillcolor=colors['hidden2'])\n",
        "        c.node('h2_etc', label='...', style='filled', fillcolor='white')\n",
        "\n",
        "    # Third Hidden Layer (16 neurons)\n",
        "    with dot.subgraph(name='cluster_hidden3') as c:\n",
        "        c.attr(label='Third Hidden Layer\\n(16 neurons, ReLU)', color='gray')\n",
        "        for i in range(4):  # Show first 4 neurons\n",
        "            c.node(f'h3_{i}', label=f'H3-{i+1}', fillcolor=colors['hidden3'])\n",
        "        c.node('h3_etc', label='...', style='filled', fillcolor='white')\n",
        "\n",
        "    # Output Layer\n",
        "    dot.node('output', label='Predicted Price', fillcolor=colors['output'])\n",
        "\n",
        "    # Connectivity (simplified)\n",
        "    # Input to First Hidden Layer\n",
        "    dot.edge('x1', 'h1_0')\n",
        "    dot.edge('x2', 'h1_1')\n",
        "    dot.edge('x_etc', 'h1_etc', style='dashed')\n",
        "\n",
        "    # First to Second Hidden Layer\n",
        "    dot.edge('h1_0', 'h2_0')\n",
        "    dot.edge('h1_1', 'h2_1')\n",
        "    dot.edge('h1_etc', 'h2_etc', style='dashed')\n",
        "\n",
        "    # Second to Third Hidden Layer\n",
        "    dot.edge('h2_0', 'h3_0')\n",
        "    dot.edge('h2_1', 'h3_1')\n",
        "    dot.edge('h2_etc', 'h3_etc', style='dashed')\n",
        "\n",
        "    # Third Hidden Layer to Output\n",
        "    for i in range(2):\n",
        "        dot.edge(f'h3_{i}', 'output')\n",
        "    dot.edge('h3_etc', 'output', style='dashed')\n",
        "\n",
        "    # Regularization annotation\n",
        "    dot.node('regularization',\n",
        "             label='Regularization:\\n- L2 Weight Decay (Œª=0.001)\\n- Dropout (30%)\\n- Batch Normalization',\n",
        "             shape='note',\n",
        "             color='purple')\n",
        "    dot.edge('output', 'regularization', style='dashed')\n",
        "\n",
        "    # Save and return the diagram\n",
        "    dot.render('house_price_prediction_nn', cleanup=True)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(plt.imread('house_price_prediction_nn.png'))\n",
        "    plt.axis('off')\n",
        "    plt.title('Neural Network Architecture for House Price Prediction')\n",
        "    plt.show()\n",
        "\n",
        "# Create and display the diagram\n",
        "create_house_price_nn_diagram()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZAEpDue2Ao"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('housing.csv')\n",
        "\n",
        "# Explore the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nCheck for missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values (impute for simplicity)\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# One-hot encode categorical columns (e.g., 'ocean_proximity')\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "# Split features and target\n",
        "X = df_encoded.drop('median_house_value', axis=1)\n",
        "y = df_encoded['median_house_value']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Linear Regression model as baseline\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the Linear Regression model\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics for Linear Regression\n",
        "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
        "lr_r2 = r2_score(y_test, y_pred_lr)\n",
        "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "# Print results for Linear Regression\n",
        "print(\"\\n------------------ Linear Regression Results ------------------\")\n",
        "print(f\"MSE: {lr_mse:.4f}\")\n",
        "print(f\"R¬≤: {lr_r2:.4f}\")\n",
        "print(f\"MAE: {lr_mae:.4f}\")\n",
        "\n",
        "# Built Neural Network Model (Example for comparison)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Create the neural network model\n",
        "def create_neural_network(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "    return model\n",
        "\n",
        "# Train the neural network model\n",
        "model = create_neural_network(X_train_scaled.shape[1])\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "# Predict with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics for Neural Network\n",
        "nn_mse = mean_squared_error(y_test, y_pred_nn)\n",
        "nn_r2 = r2_score(y_test, y_pred_nn)\n",
        "nn_mae = mean_absolute_error(y_test, y_pred_nn)\n",
        "\n",
        "# Print results for Neural Network\n",
        "print(\"\\n------------------ Neural Network Results ------------------\")\n",
        "print(f\"MSE: {nn_mse:.4f}\")\n",
        "print(f\"R¬≤: {nn_r2:.4f}\")\n",
        "print(f\"MAE: {nn_mae:.4f}\")\n",
        "\n",
        "# Plot Learning Curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loss plot for Neural Network\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# MAE plot for Neural Network\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mean_absolute_error'], label='Train MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize correlation with median house value\n",
        "plt.figure(figsize=(12, 8))\n",
        "corr_matrix = df_encoded.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Features Most Correlated with House Price\n",
        "corr_with_price = corr_matrix['median_house_value'].sort_values(ascending=False)\n",
        "print(\"\\nTop Features Most Correlated with House Price:\")\n",
        "print(corr_with_price.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_RpCR-id452"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWSIPyWpd34O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbbdJ4kCrr3F"
      },
      "source": [
        "Here's the updated **Fraud Detection Model Performance Analysis** for your regression-based house price prediction task:\n",
        "\n",
        "---\n",
        "\n",
        "## üïµÔ∏è House Price Prediction Model Performance Analysis\n",
        "\n",
        "### üîç Key Performance Metrics Comparison\n",
        "\n",
        "| Metric                | Linear Regression  | Neural Network  | Improvement   |\n",
        "|-----------------------|--------------------|-----------------|---------------|\n",
        "| **MSE**               | 4,904,399,775.95   | 4,533,685,388.79| ‚Üì 7.55%       |\n",
        "| **R¬≤**                | 0.6257             | 0.6540          | ‚Üë 2.81%       |\n",
        "| **MAE**               | 50,701.7790        | 46,388.8714     | ‚Üë 8.5%        |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Detailed Performance Breakdown\n",
        "\n",
        "### 1Ô∏è‚É£ **Mean Squared Error (MSE)**\n",
        "- **Linear Regression**: MSE = 4,904,399,775.95\n",
        "- **Neural Network**: MSE = 4,533,685,388.79\n",
        "- **Improvement**: The Neural Network reduces MSE by **7.55%**, indicating a better fit to the data.\n",
        "\n",
        "### 2Ô∏è‚É£ **R¬≤ (Coefficient of Determination)**\n",
        "- **Linear Regression**: R¬≤ = 0.6257\n",
        "- **Neural Network**: R¬≤ = 0.6540\n",
        "- **Improvement**: The Neural Network shows an **2.81%** improvement, meaning it explains more of the variance in house prices.\n",
        "\n",
        "### 3Ô∏è‚É£ **Mean Absolute Error (MAE)**\n",
        "- **Linear Regression**: MAE = 50,701.7790\n",
        "- **Neural Network**: MAE = 46,388.8714\n",
        "- **Improvement**: The Neural Network improves MAE by **8.5%**, meaning it provides predictions that are closer to the actual values.\n",
        "\n",
        "---\n",
        "\n",
        "## üö® Key Insights\n",
        "\n",
        "### **Advantages of Neural Network**\n",
        "1. **Better model fit**: The Neural Network shows improvement in R¬≤ and MSE, indicating it captures the underlying data patterns better than linear regression.\n",
        "2. **Lower MAE**: Neural Network produces predictions closer to actual values, reducing prediction errors.\n",
        "3. **Non-linear feature interactions**: The Neural Network is better suited for capturing complex, non-linear relationships between features.\n",
        "\n",
        "### **Challenges**\n",
        "1. **Increased complexity**: The Neural Network is more computationally intensive, making it harder to interpret and requiring more resources to train and deploy.\n",
        "2. **Risk of overfitting**: Though improved, the complexity of the model could lead to overfitting if not properly regularized and validated.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Practical Recommendations\n",
        "1. **Use Neural Network for improved accuracy**: Given its performance, the Neural Network should be the preferred model for house price prediction.\n",
        "2. **Feature Engineering**: Investigate how adding or modifying features (e.g., incorporating interaction terms or non-linear transformations) could further improve model performance.\n",
        "3. **Regularization and tuning**: Regularization techniques like dropout and L2 regularization, along with tuning the learning rate, will ensure that the Neural Network doesn't overfit.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è **Conclusion**\n",
        "- The **Neural Network** offers a more accurate and reliable model for predicting house prices compared to **Linear Regression**.\n",
        "- While the model is more complex, it provides better results, especially in terms of prediction accuracy and error reduction.\n",
        "- Given the model's superior performance, the **Neural Network** is recommended for practical house price prediction applications.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **The Neural Network provides a better fit to the house price prediction task, offering improved performance and more reliable predictions compared to Linear Regression.**\n",
        "\n",
        "---\n",
        "\n",
        "### Top Features Most Correlated with House Price:\n",
        "\n",
        "| Feature                            | Correlation Coefficient |\n",
        "|------------------------------------|-------------------------|\n",
        "| **median_house_value**             | 1.0000                  |\n",
        "| **median_income**                  | 0.6881                  |\n",
        "| **ocean_proximity_NEAR BAY**       | 0.1603                  |\n",
        "| **ocean_proximity_NEAR OCEAN**     | 0.1419                  |\n",
        "| **total_rooms**                    | 0.1342                  |\n",
        "| **housing_median_age**             | 0.1056                  |\n",
        "| **households**                     | 0.0658                  |\n",
        "| **total_bedrooms**                 | 0.0495                  |\n",
        "| **ocean_proximity_ISLAND**         | 0.0234                  |\n",
        "| **population**                     | -0.0247                 |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrCs9_YWEQbc"
      },
      "outputs": [],
      "source": [
        "# Assuming `df` is your training data, and `new_data` is the new data for prediction\n",
        "\n",
        "# Example: one-hot encode categorical variable 'ocean_proximity'\n",
        "new_data = pd.DataFrame({\n",
        "    'latitude': [34.0],\n",
        "    'longitude': [-118.0],\n",
        "    'median_income': [6.0],\n",
        "    'housing_median_age': [20],\n",
        "    'households': [400],\n",
        "    'total_bedrooms': [200],\n",
        "    'total_rooms': [1500],\n",
        "    'ocean_proximity': ['NEAR BAY']\n",
        "})\n",
        "\n",
        "# One-hot encode the categorical 'ocean_proximity' as was done during training\n",
        "new_data = pd.get_dummies(new_data, columns=['ocean_proximity'], drop_first=True)\n",
        "\n",
        "# Align the columns with the training data (fill missing columns with 0)\n",
        "missing_cols = set(X_train.columns) - set(new_data.columns)\n",
        "for col in missing_cols:\n",
        "    new_data[col] = 0\n",
        "\n",
        "# Reorder columns to match the training data\n",
        "new_data = new_data[X_train.columns]\n",
        "\n",
        "# Standardize features using the same scaler used for training\n",
        "new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "# Now you can make predictions\n",
        "lr_prediction = lr.predict(new_data_scaled)\n",
        "print(f\"Linear Regression Predicted Price: {lr_prediction[0]}\")\n",
        "\n",
        "nn_prediction = model.predict(new_data_scaled)\n",
        "print(f\"Neural Network Predicted Price: {nn_prediction[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl7xwGJNEv55"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('housing.csv')\n",
        "\n",
        "# Explore the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nCheck for missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values (impute for simplicity)\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# One-hot encode categorical columns (e.g., 'ocean_proximity')\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Split features and target\n",
        "X = df_encoded.drop('median_house_value', axis=1)\n",
        "y = df_encoded['median_house_value']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Linear Regression model as baseline\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the Linear Regression model\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics for Linear Regression\n",
        "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
        "lr_r2 = r2_score(y_test, y_pred_lr)\n",
        "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "# Print results for Linear Regression\n",
        "print(\"\\n------------------ Linear Regression Results ------------------\")\n",
        "print(f\"MSE: {lr_mse:.4f}\")\n",
        "print(f\"R¬≤: {lr_r2:.4f}\")\n",
        "print(f\"MAE: {lr_mae:.4f}\")\n",
        "\n",
        "# Create the neural network model\n",
        "def create_neural_network(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)  # Output layer for regression\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "    return model\n",
        "\n",
        "# Train the neural network model\n",
        "model = create_neural_network(X_train_scaled.shape[1])\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "# Predict with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics for Neural Network\n",
        "nn_mse = mean_squared_error(y_test, y_pred_nn)\n",
        "nn_r2 = r2_score(y_test, y_pred_nn)\n",
        "nn_mae = mean_absolute_error(y_test, y_pred_nn)\n",
        "\n",
        "# Print results for Neural Network\n",
        "print(\"\\n------------------ Neural Network Results ------------------\")\n",
        "print(f\"MSE: {nn_mse:.4f}\")\n",
        "print(f\"R¬≤: {nn_r2:.4f}\")\n",
        "print(f\"MAE: {nn_mae:.4f}\")\n",
        "\n",
        "# Plot Learning Curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loss plot for Neural Network\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# MAE plot for Neural Network\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mean_absolute_error'], label='Train MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize correlation with median house value\n",
        "plt.figure(figsize=(12, 8))\n",
        "corr_matrix = df_encoded.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Features Most Correlated with House Price\n",
        "corr_with_price = corr_matrix['median_house_value'].sort_values(ascending=False)\n",
        "print(\"\\nTop Features Most Correlated with House Price:\")\n",
        "print(corr_with_price.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Create sliders using 1st to 99th percentile to avoid outliers\n",
        "sliders = {}\n",
        "for feature in X_train.columns:\n",
        "    if X_train[feature].dtype == 'object' or X_train[feature].dtype == 'bool':\n",
        "        unique_values = X_train[feature].dropna().unique().tolist()\n",
        "        sliders[feature] = widgets.Dropdown(\n",
        "            options=unique_values,\n",
        "            value=unique_values[0],\n",
        "            description=feature.replace('_', ' ').capitalize()\n",
        "        )\n",
        "    else:\n",
        "        q1 = np.percentile(X_train[feature].dropna(), 1)\n",
        "        q99 = np.percentile(X_train[feature].dropna(), 99)\n",
        "        mean_val = X_train[feature].mean()\n",
        "        step_val = (q99 - q1) / 100 if q99 != q1 else 0.01\n",
        "\n",
        "        sliders[feature] = widgets.FloatSlider(\n",
        "            value=np.clip(mean_val, q1, q99),\n",
        "            min=q1,\n",
        "            max=q99,\n",
        "            step=step_val,\n",
        "            description=feature.replace('_', ' ').capitalize(),\n",
        "            continuous_update=False,\n",
        "            readout_format='.2f',\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=widgets.Layout(width='80%')\n",
        "        )\n",
        "\n",
        "# Output area for prediction results\n",
        "output = widgets.Output()\n",
        "\n",
        "# Prediction button\n",
        "predict_button = widgets.Button(description=\"Predict\", button_style='success')\n",
        "\n",
        "# Predict function\n",
        "def on_predict_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        new_data = pd.DataFrame({f: [sliders[f].value] if not isinstance(sliders[f], widgets.Dropdown) else [sliders[f].value] for f in X_train.columns})\n",
        "\n",
        "        # Handle categorical encoding\n",
        "        if 'ocean_proximity' in new_data.columns:\n",
        "            new_data = pd.get_dummies(new_data, columns=['ocean_proximity'], drop_first=True)\n",
        "\n",
        "        # Add missing columns\n",
        "        for col in X_train.columns:\n",
        "            if col not in new_data.columns:\n",
        "                new_data[col] = 0\n",
        "\n",
        "        new_data = new_data[X_train.columns]  # Reorder\n",
        "        new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "        # Predictions\n",
        "        lr_prediction = lr.predict(new_data_scaled)\n",
        "        nn_prediction = model.predict(new_data_scaled)\n",
        "\n",
        "        print(f\"üîπ Linear Regression Predicted Price: ${lr_prediction[0]:,.2f}\")\n",
        "        print(f\"üîπ Neural Network Predicted Price: ${nn_prediction[0][0]:,.2f}\")\n",
        "\n",
        "predict_button.on_click(on_predict_clicked)\n",
        "\n",
        "# Dashboard layout\n",
        "dashboard = widgets.VBox([\n",
        "    widgets.HTML(\"<h1 style='text-align:center; color:#2c3e50;'>üè† House Price Prediction Dashboard</h1>\"),\n",
        "    widgets.HTML(\"<div style='text-align:center'><img src='https://images.pexels.com/photos/271639/pexels-photo-271639.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=450&w=800' width='500'></div>\"),\n",
        "    widgets.HTML(\"<h3 style='text-align:center;'>Adjust the features below to predict the price</h3>\"),\n",
        "    *sliders.values(),\n",
        "    widgets.HTML(\"<br>\"),\n",
        "    predict_button,\n",
        "    output\n",
        "], layout=widgets.Layout(align_items='center', width='60%', margin='0 auto'))\n",
        "\n",
        "# Display centered\n",
        "display(HTML(\"<div style='display:flex; justify-content:center;'>\"))\n",
        "display(dashboard)\n",
        "display(HTML(\"</div>\"))\n"
      ],
      "metadata": {
        "id": "3Qx1h13tjsMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}